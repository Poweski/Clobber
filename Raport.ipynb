{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb99c34d-2aee-40a2-b505-f69bc104f0ed",
   "metadata": {},
   "source": [
    "## Raport: Implementacja Gry Clobber z AI\n",
    "\n",
    "**Autor:** Jan Powęski\n",
    "**Data:** 28.05.2025\n",
    "\n",
    "### 1. Wprowadzenie\n",
    "\n",
    "Niniejszy raport dotyczy analizy i opisu programu `clobber.py`, który implementuje grę logiczną Clobber oraz agentów sztucznej inteligencji (AI) do rozgrywki. Clobber to gra dla dwóch graczy, rozgrywana na prostokątnej planszy, na której początkowo wszystkie pola są zajęte przez pionki dwóch kolorów. Celem gry jest zbicie wszystkich pionków przeciwnika lub doprowadzenie do sytuacji, w której przeciwnik nie może wykonać ruchu.\n",
    "\n",
    "Program `clobber.py` implementuje logikę gry, algorytmy przeszukiwania drzewa gry (Minimax i Alfa-Beta) oraz różne funkcje heurystyczne do oceny stanu gry. Umożliwia rozgrywkę w dwóch trybach: \"basic\" (Minimax vs Minimax z adaptacyjną heurystyką) oraz \"extended\" (konfigurowalne agenty dla każdego gracza).\n",
    "\n",
    "### 2. Opis Teoretyczny Metody\n",
    "\n",
    "Program wykorzystuje klasyczne algorytmy przeszukiwania z teorii gier do podejmowania decyzji przez agentów AI.\n",
    "\n",
    "#### 2.1. Algorytm Minimax\n",
    "\n",
    "Minimax to algorytm rekurencyjny używany w teorii gier do wyboru optymalnego ruchu dla gracza, zakładając, że przeciwnik również gra optymalnie. Działa poprzez budowę drzewa gry, gdzie węzły reprezentują stany gry, a krawędzie to możliwe ruchy.\n",
    "*   **Gracz MAX:** Gracz, dla którego aktualnie szukamy najlepszego ruchu (starający się zmaksymalizować swoją ocenę).\n",
    "*   **Gracz MIN:** Przeciwnik (starający się zminimalizować ocenę gracza MAX).\n",
    "Algorytm przeszukuje drzewo do określonej głębokości. W liściach drzewa (lub na osiągniętej głębokości) stosowana jest **funkcja heurystyczna**, która ocenia, jak korzystny jest dany stan gry dla gracza MAX. Wartości te są następnie propagowane w górę drzewa:\n",
    "*   Węzły MAX wybierają maksymalną wartość spośród swoich dzieci.\n",
    "*   Węzły MIN wybierają minimalną wartość spośród swoich dzieci.\n",
    "Ostatecznie, korzeń drzewa otrzymuje wartość, która reprezentuje ocenę najlepszego możliwego do osiągnięcia stanu po wykonaniu optymalnego ruchu, a sam ruch jest również zwracany.\n",
    "\n",
    "#### 2.2. Algorytm Alfa-Beta (Alpha-Beta Pruning)\n",
    "\n",
    "Alfa-Beta to optymalizacja algorytmu Minimax. Redukuje liczbę przeszukiwanych węzłów w drzewie gry, odcinając gałęzie, które na pewno nie wpłyną na ostateczną decyzję. Działa poprzez utrzymywanie dwóch wartości podczas przeszukiwania:\n",
    "*   **Alfa (α):** Najlepsza (najwyższa) wartość znaleziona do tej pory dla gracza MAX na ścieżce do korzenia.\n",
    "*   **Beta (β):** Najlepsza (najniższa) wartość znaleziona do tej pory dla gracza MIN na ścieżce do korzenia.\n",
    "Odcięcie następuje, gdy:\n",
    "*   Dla węzła MIN, jego tymczasowa wartość staje się mniejsza lub równa α ( `wartość <= α` ). Oznacza to, że gracz MAX ma już lepszą alternatywę w innej części drzewa i nie wybierze tej gałęzi.\n",
    "*   Dla węzła MAX, jego tymczasowa wartość staje się większa lub równa β ( `wartość >= β` ). Oznacza to, że gracz MIN ma już lepszą alternatywę (dla siebie) w innej części drzewa i nie pozwoli graczowi MAX osiągnąć tej gałęzi.\n",
    "Alfa-Beta daje ten sam wynik co Minimax, ale jest znacznie wydajniejszy, zwłaszcza dla głębokich drzew.\n",
    "\n",
    "#### 2.3. Funkcje Heurystyczne\n",
    "\n",
    "Ponieważ pełne przeszukanie drzewa gry Clobber jest obliczeniowo niewykonalne (zbyt duża liczba możliwych stanów), algorytmy przeszukują do ograniczonej głębokości. Funkcje heurystyczne służą do oceny \"jakości\" stanów gry na tej granicy przeszukiwania. Dobra heurystyka powinna szybko ocenić, który gracz ma przewagę, bez konieczności dalszego rozwijania drzewa. W kodzie zaimplementowano kilka heurystyk, np.:\n",
    "*   Różnica w liczbie pionków.\n",
    "*   Różnica w liczbie możliwych ruchów (mobilność).\n",
    "*   Liczba bezpiecznych pionków.\n",
    "*   Kontrola centrum planszy.\n",
    "Implementacja zawiera również mechanizm **heurystyki adaptacyjnej**, która wybiera jedną z predefiniowanych heurystyk w zależności od fazy gry (np. liczby pozostałych pionków na planszy).\n",
    "\n",
    "### 3. Formalne Sformułowanie Problemu\n",
    "\n",
    "#### 3.1. Gra Clobber\n",
    "\n",
    "*   **Gracze:** Dwóch graczy, w kodzie oznaczonych jako `PLAYER_B` ('B') i `PLAYER_W` ('W').\n",
    "*   **Plansza:** Prostokątna siatka o wymiarach `ROWS` x `COLS` (w kodzie 6x5). Każde pole jest albo puste (`EMPTY` = '_') albo zajęte przez pionek jednego z graczy.\n",
    "*   **Stan Gry (S):** Reprezentowany przez dwuwymiarową tablicę (listę list w Pythonie) zawierającą konfigurację pionków na planszy.\n",
    "    `S = board[ROWS][COLS]`, gdzie `board[r][c] ∈ {PLAYER_B, PLAYER_W, EMPTY}`.\n",
    "*   **Akcje (Ruchy - A(s)):** Gracz może przesunąć swój pionek na sąsiednie (ortogonalnie: góra, dół, lewo, prawo) pole zajęte przez pionek przeciwnika. Pionek przeciwnika jest zdejmowany z planszy (zbity), a pionek gracza zajmuje jego miejsce.\n",
    "    Formalnie, ruch `m = ((r1, c1), (r2, c2))` jest legalny, jeśli:\n",
    "    1.  `board[r1][c1]` == `aktualny_gracz`\n",
    "    2.  `board[r2][c2]` == `przeciwnik(aktualny_gracz)`\n",
    "    3.  `(r2, c2)` jest sąsiadem ortogonalnym `(r1, c1)`: `|r1-r2| + |c1-c2| = 1`.\n",
    "*   **Funkcja Przejścia (T(s, a) -> s'):** `apply_move(board, move)` zwraca nowy stan planszy `s'` po wykonaniu ruchu `a` w stanie `s`.\n",
    "*   **Stan Początkowy (S₀):** Konfiguracja planszy wczytywana z wejścia (np. pliku `board.txt`). W typowej grze Clobber plansza jest na początku całkowicie wypełniona naprzemiennie pionkami graczy.\n",
    "*   **Warunki Zakończenia Gry (Stany Terminalne):**\n",
    "    1.  Gracz zbije wszystkie pionki przeciwnika. Ten gracz wygrywa.\n",
    "    2.  Gracz nie może wykonać żadnego legalnego ruchu (brak pionków, które mogą się ruszyć na pole przeciwnika). Ten gracz przegrywa (przeciwnik wygrywa).\n",
    "*   **Cel Gry:** Doprowadzić do stanu terminalnego, w którym jest się zwycięzcą.\n",
    "\n",
    "#### 3.2. Ograniczenia\n",
    "\n",
    "*   **Rozmiar planszy:** `ROWS = 6`, `COLS = 5`.\n",
    "*   **Głębokość przeszukiwania (Depth):** `GAME_DEPTH = 3` (domyślnie, może być zmieniona w `AGENT_SETTINGS`). Jest to ograniczenie nałożone na algorytmy Minimax/Alfa-Beta, aby zapewnić rozsądny czas odpowiedzi.\n",
    "*   **Czas:** Choć nie ma twardego limitu czasu na ruch w kodzie, praktycznie głębokość przeszukiwania i złożoność heurystyk wpływają na czas wykonania.\n",
    "\n",
    "#### 3.3. Drzewo Decyzyjne (Drzewo Gry)\n",
    "\n",
    "*   **Korzeń:** Aktualny stan gry.\n",
    "*   **Węzły:** Możliwe stany gry.\n",
    "*   **Krawędzie:** Legalne ruchy prowadzące z jednego stanu do drugiego.\n",
    "*   **Poziomy:** Na przemian reprezentują tury gracza MAX (np. 'B') i MIN (np. 'W').\n",
    "*   **Liście:**\n",
    "    *   Stany terminalne (zwycięstwo, porażka).\n",
    "    *   Stany osiągnięte na maksymalnej głębokości przeszukiwania. Wartość tych liści jest szacowana przez funkcję heurystyczną.\n",
    "\n",
    "Celem agenta AI jest znalezienie ścieżki od korzenia do liścia (w ramach ustalonej głębokości), która maksymalizuje jego funkcję użyteczności (heurystykę), zakładając, że przeciwnik będzie minimalizował tę samą funkcję użyteczności.\n",
    "\n",
    "### 4. Opis Idei Rozwiązania (Implementacja w `clobber.py`)\n",
    "\n",
    "Program implementuje grę Clobber i agentów AI w następujący sposób:\n",
    "\n",
    "1.  **Reprezentacja Planszy i Podstawowe Operacje:**\n",
    "    *   Plansza jest listą list (`board`).\n",
    "    *   Stałe `PLAYER_B`, `PLAYER_W`, `EMPTY` definiują zawartość pól.\n",
    "    *   `DIRECTIONS` ułatwia generowanie sąsiednich pól.\n",
    "    *   `read_board()` wczytuje stan początkowy.\n",
    "    *   `print_board_to_stdout()` wyświetla planszę.\n",
    "    *   `get_opponent()` zwraca symbol przeciwnika.\n",
    "\n",
    "2.  **Logika Gry:**\n",
    "    *   `generate_moves(board, player)`: Kluczowa funkcja, która dla danego gracza i stanu planszy generuje listę wszystkich możliwych legalnych ruchów. Iteruje po wszystkich polach, sprawdza, czy należy do gracza, a następnie sprawdza sąsiednie pola w poszukiwaniu pionków przeciwnika.\n",
    "    *   `apply_move(board, move)`: Tworzy kopię planszy (aby nie modyfikować oryginalnej podczas przeszukiwania) i wykonuje na niej ruch, zmieniając `board[r1][c1]` na `EMPTY` i `board[r2][c2]` na pionek gracza, który wykonał ruch.\n",
    "\n",
    "3.  **Funkcje Heurystyczne:**\n",
    "    *   Zdefiniowano zestaw funkcji heurystycznych (`heuristic_B_1`, `heuristic_B_2`, ..., `heuristic_W_3`). Każda z nich przyjmuje `board` i `player_perspective` (gracza, z perspektywy którego oceniamy stan) i zwraca wartość liczbową.\n",
    "        *   `heuristic_B_1`: Różnica w liczbie pionków. Prosta, ale często skuteczna.\n",
    "        *   `heuristic_B_2`: Różnica w mobilności (liczbie możliwych ruchów).\n",
    "        *   `heuristic_B_3`: Liczba własnych ruchów.\n",
    "        *   `heuristic_W_1`: Liczba \"bezpiecznych\" pionków (niezagrożonych bezpośrednim zbiciem).\n",
    "        *   `heuristic_W_2`: Ujemna liczba ruchów przeciwnika (im mniej ruchów ma przeciwnik, tym lepiej).\n",
    "        *   `heuristic_W_3`: Kontrola centralnych pól planszy.\n",
    "    *   `HEURISTICS`: Słownik mapujący gracza i klucz heurystyki na odpowiednią funkcję.\n",
    "    *   `choose_adaptive_heuristic(board, player)`: Wybiera heurystykę ('1', '2' lub '3') na podstawie liczby pionków na planszy, próbując dostosować strategię do fazy gry (początkowa, środkowa, końcowa).\n",
    "\n",
    "4.  **Algorytmy Przeszukiwania:**\n",
    "    *   `minimax(board, depth, maximizing_turn, root_player, heuristic_for_root_player)`:\n",
    "        *   Implementuje algorytm Minimax.\n",
    "        *   `depth`: Aktualna głębokość rekurencji.\n",
    "        *   `maximizing_turn`: `True`, jeśli to tura gracza MAX, `False` dla MIN.\n",
    "        *   `root_player`: Gracz, dla którego pierwotnie wywołano funkcję (z perspektywy którego liczymy heurystykę).\n",
    "        *   `heuristic_for_root_player`: Funkcja heurystyczna do użycia.\n",
    "        *   Rekurencyjnie wywołuje siebie dla każdego możliwego ruchu, zmniejszając `depth` i zmieniając `maximizing_turn`.\n",
    "        *   Zwraca `(ocena, najlepszy_ruch, liczba_odwiedzonych_węzłów)`.\n",
    "    *   `alphabeta(board, depth, alpha, beta, maximizing_turn, root_player, heuristic_for_root_player)`:\n",
    "        *   Implementuje algorytm Alfa-Beta.\n",
    "        *   `alpha`, `beta`: Wartości dla odcięć.\n",
    "        *   Działa podobnie do Minimax, ale przekazuje i aktualizuje `alpha` i `beta`, dokonując odcięć, gdy `beta <= alpha`.\n",
    "\n",
    "5.  **Struktura Rozgrywki:**\n",
    "    *   **Tryb \"basic\" (`play_basic_minimax_vs_minimax`):**\n",
    "        *   Obaj gracze używają algorytmu Minimax.\n",
    "        *   Heurystyka jest wybierana adaptacyjnie (`choose_adaptive_heuristic`) dla każdego gracza w jego turze.\n",
    "        *   Gra toczy się do momentu, gdy jeden z graczy nie ma ruchów lub nie ma pionków.\n",
    "    *   **Tryb \"extended\" (`play_extended_agents_game`):**\n",
    "        *   Wykorzystuje klasę `Agent`. Każdy agent (`agent_B`, `agent_W`) jest konfigurowany przy starcie (algorytm, klucz heurystyki, głębokość) za pomocą słowników `AGENT_B_SETTINGS` i `AGENT_W_SETTINGS`.\n",
    "        *   Agent posiada metodę `get_move_and_stats(board)`, która:\n",
    "            1.  Wybiera funkcję heurystyczną (może być \"adaptive\" lub stała).\n",
    "            2.  Wywołuje odpowiedni algorytm (Minimax lub Alfa-Beta).\n",
    "            3.  Zwraca `(ocena, ruch, liczba_węzłów, czas_tura)`.\n",
    "        *   Pętla gry zarządza turami, wywołując agentów, aktualizując planszę i sprawdzając warunki końca gry.\n",
    "    *   `main()`: Odczytuje planszę, a następnie na podstawie `GAME_VERSION_CHOICE` uruchamia odpowiedni tryb gry.\n",
    "\n",
    "6.  **Zbieranie Statystyk:**\n",
    "    *   Oba tryby gry zliczają całkowitą liczbę odwiedzonych węzłów w drzewie gry oraz całkowity czas poświęcony na obliczenia przez AI. Te informacje są wypisywane na `sys.stderr`.\n",
    "\n",
    "### 5. Krótki Przykład w Jupyter Notebook\n",
    "\n",
    "Poniżej znajduje się fragment kodu, który można uruchomić w Jupyter Notebook, aby zademonstrować działanie podstawowych funkcji i wykonanie jednego ruchu przez agenta AI. Dla uproszczenia, kluczowe funkcje i stałe zostały skopiowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82e1dee-ae37-48dd-bad5-5116cb209897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definicje wczytane.\n"
     ]
    }
   ],
   "source": [
    "# Komórka 1: Definicje i funkcje (skopiowane/zaadaptowane z clobber.py)\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# --- Constants ---\n",
    "PLAYER_B = 'B'\n",
    "PLAYER_W = 'W'\n",
    "EMPTY = '_'\n",
    "DIRECTIONS = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "# Użyjemy mniejszej planszy i głębokości dla przykładu\n",
    "COLS, ROWS = 3, 2 \n",
    "GAME_DEPTH_EXAMPLE = 2\n",
    "\n",
    "# --- Board Utilities ---\n",
    "def print_board(board):\n",
    "    for row in board:\n",
    "        print(' '.join(row))\n",
    "\n",
    "def get_opponent(player):\n",
    "    return PLAYER_W if player == PLAYER_B else PLAYER_B\n",
    "\n",
    "def generate_moves(board, player, current_rows, current_cols):\n",
    "    moves = []\n",
    "    for r in range(current_rows):\n",
    "        for c in range(current_cols):\n",
    "            if board[r][c] == player:\n",
    "                for dr, dc in DIRECTIONS:\n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    if 0 <= nr < current_rows and 0 <= nc < current_cols and board[nr][nc] == get_opponent(player):\n",
    "                        moves.append(((r, c), (nr, nc)))\n",
    "    return moves\n",
    "\n",
    "def apply_move(board, move):\n",
    "    (r1, c1), (r2, c2) = move\n",
    "    new_board = copy.deepcopy(board)\n",
    "    new_board[r2][c2] = new_board[r1][c1]\n",
    "    new_board[r1][c1] = EMPTY\n",
    "    return new_board\n",
    "\n",
    "# --- Heuristic Example (B_2 from the code) ---\n",
    "def heuristic_example_mobility(board, player_perspective, current_rows, current_cols):\n",
    "    opp = get_opponent(player_perspective)\n",
    "    # Pass current_rows and current_cols to generate_moves\n",
    "    return len(generate_moves(board, player_perspective, current_rows, current_cols)) - \\\n",
    "           len(generate_moves(board, opp, current_rows, current_cols))\n",
    "\n",
    "# --- Search Algorithm Example (Alpha-Beta) ---\n",
    "def alphabeta(board, depth, alpha, beta, maximizing_turn, root_player, heuristic_fn, current_rows, current_cols):\n",
    "    nodes_visited_total = 1\n",
    "    actual_mover = root_player if maximizing_turn else get_opponent(root_player)\n",
    "    moves = generate_moves(board, actual_mover, current_rows, current_cols)\n",
    "\n",
    "    if depth == 0 or not moves:\n",
    "        return heuristic_fn(board, root_player, current_rows, current_cols), None, nodes_visited_total\n",
    "\n",
    "    best_move = moves[0] if moves else None\n",
    "\n",
    "    if maximizing_turn:\n",
    "        max_eval = float('-inf')\n",
    "        for move in moves:\n",
    "            new_board = apply_move(board, move)\n",
    "            eval_val, _, nodes_subtree = alphabeta(new_board, depth - 1, alpha, beta, False, root_player, heuristic_fn, current_rows, current_cols)\n",
    "            nodes_visited_total += nodes_subtree\n",
    "            if eval_val > max_eval:\n",
    "                max_eval = eval_val\n",
    "                best_move = move\n",
    "            alpha = max(alpha, eval_val)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_move, nodes_visited_total\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for move in moves:\n",
    "            new_board = apply_move(board, move)\n",
    "            eval_val, _, nodes_subtree = alphabeta(new_board, depth - 1, alpha, beta, True, root_player, heuristic_fn, current_rows, current_cols)\n",
    "            nodes_visited_total += nodes_subtree\n",
    "            if eval_val < min_eval:\n",
    "                min_eval = eval_val\n",
    "                best_move = move\n",
    "            beta = min(beta, eval_val)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_move, nodes_visited_total\n",
    "\n",
    "print(\"Definicje wczytane.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae786e39-cd51-4ab2-a118-e42537731d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plansza początkowa:\n",
      "B W B\n",
      "W B W\n",
      "\n",
      "Rozmiar planszy: ROWS=2, COLS=3\n",
      "\n",
      "Gracz wykonujący ruch: B\n",
      "Możliwe ruchy dla B: [((0, 0), (1, 0)), ((0, 0), (0, 1)), ((0, 2), (1, 2)), ((0, 2), (0, 1)), ((1, 1), (0, 1)), ((1, 1), (1, 2)), ((1, 1), (1, 0))]\n",
      "\n",
      "--- Wynik działania Alpha-Beta (głębokość: 2) ---\n",
      "Najlepszy ruch dla B: ((0, 0), (1, 0))\n",
      "Ocena heurystyczna tego ruchu: 0\n",
      "Liczba odwiedzonych węzłów: 18\n",
      "Czas obliczeń: 0.0003s\n",
      "\n",
      "Plansza po wykonaniu najlepszego ruchu:\n",
      "_ W B\n",
      "B B W\n"
     ]
    }
   ],
   "source": [
    "# Komórka 2: Przygotowanie i uruchomienie przykładu\n",
    "\n",
    "# Prosta plansza 2x3\n",
    "# B W B\n",
    "# W B W\n",
    "initial_board_example = [\n",
    "    [PLAYER_B, PLAYER_W, PLAYER_B],\n",
    "    [PLAYER_W, PLAYER_B, PLAYER_W]\n",
    "]\n",
    "CURRENT_ROWS, CURRENT_COLS = 2, 3 # Zgodnie z initial_board_example\n",
    "\n",
    "print(\"Plansza początkowa:\")\n",
    "print_board(initial_board_example)\n",
    "print(f\"\\nRozmiar planszy: ROWS={CURRENT_ROWS}, COLS={CURRENT_COLS}\")\n",
    "\n",
    "# Kto gra?\n",
    "current_player_example = PLAYER_B\n",
    "print(f\"\\nGracz wykonujący ruch: {current_player_example}\")\n",
    "\n",
    "# Wygenerujmy możliwe ruchy dla gracza B\n",
    "possible_moves_b = generate_moves(initial_board_example, current_player_example, CURRENT_ROWS, CURRENT_COLS)\n",
    "print(f\"Możliwe ruchy dla {current_player_example}: {possible_moves_b}\")\n",
    "\n",
    "# Użyjmy Alpha-Beta do znalezienia najlepszego ruchu dla gracza B\n",
    "# z głębokością GAME_DEPTH_EXAMPLE i heurystyką heuristic_example_mobility\n",
    "start_time = time.perf_counter()\n",
    "eval_score, best_move, nodes = alphabeta(\n",
    "    initial_board_example, \n",
    "    GAME_DEPTH_EXAMPLE, \n",
    "    float('-inf'), \n",
    "    float('inf'), \n",
    "    True,  # Maximizing turn\n",
    "    current_player_example,\n",
    "    heuristic_example_mobility,\n",
    "    CURRENT_ROWS, # Przekazanie aktualnych wymiarów\n",
    "    CURRENT_COLS  # Przekazanie aktualnych wymiarów\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"\\n--- Wynik działania Alpha-Beta (głębokość: {GAME_DEPTH_EXAMPLE}) ---\")\n",
    "print(f\"Najlepszy ruch dla {current_player_example}: {best_move}\")\n",
    "print(f\"Ocena heurystyczna tego ruchu: {eval_score}\")\n",
    "print(f\"Liczba odwiedzonych węzłów: {nodes}\")\n",
    "print(f\"Czas obliczeń: {end_time - start_time:.4f}s\")\n",
    "\n",
    "if best_move:\n",
    "    print(\"\\nPlansza po wykonaniu najlepszego ruchu:\")\n",
    "    final_board_example = apply_move(initial_board_example, best_move)\n",
    "    print_board(final_board_example)\n",
    "else:\n",
    "    print(\"\\nBrak możliwych ruchów!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac062a-639b-4450-b7ba-4684ffd053d5",
   "metadata": {},
   "source": [
    "**Wyjaśnienie przykładu z Jupyter Notebook:**\n",
    "1.  W pierwszej komórce definiujemy niezbędne stałe i funkcje skopiowane z `clobber.py`. Zmieniono `ROWS` i `COLS` na mniejsze dla szybszego działania przykładu oraz `GAME_DEPTH_EXAMPLE` również na małą wartość. Funkcje `generate_moves` i `heuristic_example_mobility` (oraz `alphabeta`) zostały dostosowane, aby przyjmować `current_rows` i `current_cols` jako argumenty, ponieważ globalne `ROWS` i `COLS` mogą być inne.\n",
    "2.  W drugiej komórce:\n",
    "    *   Tworzymy małą, przykładową planszę `initial_board_example` (2x3).\n",
    "    *   Określamy, który gracz (`current_player_example`) ma wykonać ruch.\n",
    "    *   Wyświetlamy możliwe ruchy dla tego gracza.\n",
    "    *   Wywołujemy funkcję `alphabeta` z tą planszą, graczem, zadaną głębokością i przykładową heurystyką (`heuristic_example_mobility`).\n",
    "    *   Drukujemy wynik: najlepszy znaleziony ruch, jego ocenę, liczbę odwiedzonych węzłów i czas obliczeń.\n",
    "    *   Jeśli znaleziono ruch, stosujemy go i drukujemy planszę wynikową.\n",
    "\n",
    "Ten przykład ilustruje kluczowy krok podejmowania decyzji przez agenta AI: ocenę stanu, przeszukanie drzewa gry do pewnej głębokości i wybór ruchu prowadzącego do najlepszego (według heurystyki) stanu.\n",
    "\n",
    "### 6. Podsumowanie i Wnioski\n",
    "\n",
    "Program `clobber.py` stanowi solidną implementację gry Clobber z agentami AI opartymi na algorytmach Minimax i Alfa-Beta. Zastosowanie różnych heurystyk, w tym mechanizmu adaptacyjnego, pozwala na badanie różnych strategii gry. Kod jest dobrze zorganizowany, z wyraźnym podziałem na logikę gry, algorytmy AI i funkcje pomocnicze.\n",
    "\n",
    "Możliwe kierunki dalszego rozwoju mogłyby obejmować:\n",
    "*   Implementację bardziej zaawansowanych heurystyk.\n",
    "*   Dodanie iteracyjnego pogłębiania (Iterative Deepening) dla lepszego zarządzania czasem.\n",
    "*   Stworzenie interfejsu graficznego użytkownika.\n",
    "*   Optymalizację generowania ruchów lub reprezentacji planszy dla większej wydajności.\n",
    "\n",
    "Kod dostarcza wartościowej bazy do nauki i eksperymentowania z algorytmami sztucznej inteligencji w kontekście gier planszowych.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
